{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sender import *\n",
    "#from evaluation import *\n",
    "from batch_processor import *\n",
    "from sender import send_file_to_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file processing...\n",
      "Processing test4.py...\n",
      "Mistral client initialized successfully.\n",
      "Output saved to ./generated_files/generated_test4.py\n",
      "Processing test1.py...\n",
      "Mistral client initialized successfully.\n",
      "Output saved to ./generated_files/generated_test1.py\n",
      "Processing test2.py...\n",
      "Mistral client initialized successfully.\n"
     ]
    },
    {
     "ename": "SDKError",
     "evalue": "API error occurred: Status 429\n{\"message\":\"Requests rate limit exceeded\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/itmo/diploma/1/batch_processor.py:29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Step 1: Process all .py files\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting file processing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mprocess_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile processing complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Step 2: Evaluate the generated files\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/itmo/diploma/1/batch_processor.py:20\u001b[0m, in \u001b[0;36mprocess_files\u001b[0;34m(input_dir, output_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43msend_file_to_mistral\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1TNjHnTyCxWE0K4zUfNyeGt1oMYSDCPV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/itmo/diploma/1/sender.py:19\u001b[0m, in \u001b[0;36msend_file_to_mistral\u001b[0;34m(input_file, output_file, api_key)\u001b[0m\n\u001b[1;32m     16\u001b[0m mistral \u001b[38;5;241m=\u001b[39m MistralWrapper(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Step 3: Send the raw text to the API\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m generated_code \u001b[38;5;241m=\u001b[39m \u001b[43mmistral\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Explicit cast to string\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Step 4: Save the API response as a .txt file\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_code:\n",
      "File \u001b[0;32m~/Documents/itmo/diploma/1/mistral_wrapper.py:32\u001b[0m, in \u001b[0;36mMistralWrapper.generate_completion\u001b[0;34m(self, prompt, model, temperature, top_p)\u001b[0m\n\u001b[1;32m     29\u001b[0m full_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Send the prompt to the Mistral API\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Return the content of the response\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/mistralai/fim.py:127\u001b[0m, in \u001b[0;36mFim.complete\u001b[0;34m(self, model, prompt, temperature, top_p, max_tokens, stream, stop, random_seed, suffix, min_tokens, retries, server_url, timeout_ms, http_headers)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5XX\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    126\u001b[0m     http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res_text, http_res\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    131\u001b[0m content_type \u001b[38;5;241m=\u001b[39m http_res\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n",
      "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 429\n{\"message\":\"Requests rate limit exceeded\"}"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'data/train.csv', 'test': 'data/test.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/TacoPrime/errored_python/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        \\nBelow is an instruction for a coding task. C...\n",
       "1        \\nBelow is an instruction for a coding task. C...\n",
       "2        \\nBelow is an instruction for a coding task. C...\n",
       "3        \\nBelow is an instruction for a coding task. C...\n",
       "4        \\nBelow is an instruction for a coding task. C...\n",
       "                               ...                        \n",
       "10193    \\nBelow is an instruction for a coding task. C...\n",
       "10194    \\nBelow is an instruction for a coding task. C...\n",
       "10195    \\nBelow is an instruction for a coding task. C...\n",
       "10196    \\nBelow is an instruction for a coding task. C...\n",
       "10197    \\nBelow is an instruction for a coding task. C...\n",
       "Name: train, Length: 10198, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral client initialized successfully.\n",
      "Output saved to ./generated_files/test5_fixed.py\n"
     ]
    }
   ],
   "source": [
    "send_file_to_mistral(\"./input_files/test5.py\", \"./generated_files/test5_fixed.py\", \n",
    "api_key = \"1TNjHnTyCxWE0K4zUfNyeGt1oMYSDCPV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(listitems): \n",
    "    final=[] for strchar in listitems: for letters in strchar: if letters in ('a','e','i','o','u', 'A','E','I','O','U'): strchar = strchar.replace(letters,\"\") final.append(strchar) \n",
    "    return final myfunc([\"rohan\", \"END\"]) \n",
    "# In[43] ### Response: There is an error of type 'missing_colon' on the line '# In[43]', the correct code should be '# In[43]:'. def myfunc(listitems): final=[] for strchar in listitems: for letters in strchar: if letters in ('a','e','i','o','u', 'A','E','I','O','U'): strchar = strchar.replace(letters,\"\") final.append(strchar) return final myfunc([\"rohan\", \"END\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing test1.py\n",
      "Success rate: 50.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_to_evaluate = \"./input_files\"\n",
    "evaluate_generated_files(directory_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistral_wrapper import MistralWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "wrap = MistralWrapper(\"1TNjHnTyCxWE0K4zUfNyeGt1oMYSDCPV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mistralai.sdk.Mistral at 0x7f8ec0e9df70>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "cal = Mistral('1TNjHnTyCxWE0K4zUfNyeGt1oMYSDCPV')\n",
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1837190900.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [6]\u001b[0;36m\u001b[0m\n\u001b[0;31m    prompt = \"def fibonacci(n):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "cal = Mistral('1TNjHnTyCxWE0K4zUfNyeGt1oMYSDCPV')\n",
    "model = \"codestral-latest\"\n",
    "prompt = \"def fibonacci(n):\n",
    "    # Base cases\n",
    "    if n <= 0:\n",
    "        return \"Input should be a positive integer\"\n",
    "    elif n == 1:\n",
    "        return 0\n",
    "    elif n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        # Using iteration for better performance\n",
    "        a, b = 0, 1\n",
    "        for _ in range(2, n):\n",
    "            a, b = b, a + b\n",
    "        return b\n",
    "\"\n",
    "#suffix = \"n = int(input('Enter a number: '))\\nprint(fibonacci(n))\"\n",
    "system_prompt = (\n",
    "            \"You are a professional Python developer. \"\n",
    "            \"Fix the provided code and return the fixed output only as text. \"\n",
    "            \"If presented code you find correct, then just output the initial code .\"\n",
    "        )\n",
    "response = cal.fim.complete(\n",
    "    model=model,\n",
    "    prompt=system_prompt+prompt,\n",
    "    #suffix=suffix,\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "{response.choices[0].message.content}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral client initialized successfully.\n",
      "Fixed Output:\n",
      "print('hell worldde')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Initialize the wrapper with the API key\n",
    "wr = MistralWrapper(api_key=\"1TNjHnTyCxWE0K4zUfNyeGt1oMYSDCPV\")\n",
    "    \n",
    "    # Example prompt\n",
    "prompt = \"print 'hell worldde'\"\n",
    "    \n",
    "    # Generate the fixed output\n",
    "fixed_code = wr.generate_completion(prompt)\n",
    "    \n",
    "print(\"Fixed Output:\")\n",
    "print(fixed_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print('hello world')\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrap.generate_completion(\"print 'hello worldde'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
